{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Introduction\n",
    "\n",
    "The important goal of this project is to wrangle weRateDogs Twitter data to create an interesting analysis ana visualization. This project is part of the data wraggling section of the udacity Data Analysis Nanodegree programme and is focus on data wraggling from WeRateDogs Twitter account using python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Details\n",
    "\n",
    "Normally real world data rarely comes out clean. Using python libraries, i had to gather, assess and clean the datatset given for this project, to carry out my analysis and visualization of the dataset.After fully assessing and cleaning the entire dataset, it would require exceptional effort so only a subset of its issues (eight quality issues and two tidiness issues at minimum) needed to be assessed and cleaned.\n",
    "\n",
    "The tasks for the project include:\n",
    "1. Data wraggling ,which consist of:\n",
    "     .Gathering\n",
    "     .Assessing\n",
    "     .Cleaning\n",
    "2.Storing, analyzing and visualizing the wraggled data.\n",
    "3.Reporting on my data analyses and visualizations (act_report.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering the Data\n",
    "\n",
    "The data for this project was in three different formats and they were obtained as mentioned below:\n",
    "\n",
    "Twitter Archive File-weRateDogs: This was extracted programmatically by udacity and providedas twitter_archive_enhanced.csv to use.\n",
    "\n",
    "image Predictions File: The tweet image predictions,breed of dog present in each tweet according to a nueral network. This File(image_prediction.tsv) was hosted on the Udacity's servers and downloaded programatically using the Requests library and the following URL:\n",
    "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "Twitter API & TweetJSON File: by using the supporting material provided by Udacity. I queried the  Twitter API for each tweet's JSON data using python tweepy library and stored each tweet's of the entire dataset of JSON data in a file called tweet_json.txt file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning\n",
    "\n",
    "This part of the data wraggling was divided into three parts as per the dataset and was further divided  into three steps Define, code and test blocks below it to make it easy to understand.\n",
    "\n",
    "The first step was was to create copies of all the three datam frame. so that i can do tial and error in the copied frame rather than the originals.\n",
    "\n",
    "In the twitter archive data, i changed the datatype of the timesmap and made the dog names consistent i.e.first letter capital.As mentioned earlier the standard for \"rating_denominator\" is 10, but on checking ifound out that it includes some other numbers, which could be the mis parse. So i check that the text corresponding  to those rating and notice d that few of them were analyzed incorrectlydue to the presence of another fraction in the text. I corrected the same for both the rating denominator and numerator.\n",
    "\n",
    "In the image prediction i found out that there were 66 duplicated values. i drop the duplicated data. I also changeed the column names to make it move descriptive and readable . Again the dog breed of all the three prediction column includes both upper and lowercases for the first letter. I made changes to make it consistent.\n",
    "\n",
    "In the generated tweet JSON data, the most important column was retweet_count and favourite_count, others were actually redundant as the same were present in the twitter archive data.So delete the unecessary columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Data\n",
    "\n",
    "After cleaning the data i discovered that there was no need for three datasets. All the data could be easily made into a single file. So i joined 'tweet_json', 'image_predictions' to weratedogs, to twitter_archive_master.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "A good data wraggler knows how to integrate information from multiple data sources, solving common information problems and resolve data cleaning and quality issues. A data wraggler also knows understands their data well and is always looking for ways to enrich the data. I have done this data analyses and visualization usingpython libraries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
